x = duomenys$variantas_72_x
y = duomenys$variantas_72_y

#2.1
# k = 1.622, b = 7.085, lygtis y = 1.622 * x + 7.085 
plot(x,y)
lm(y~x)
k = 1.622 
b = 7.085 
lines(x, k*x +b)

#2.2
#Tikrinsiu reikÅ¡mÄ™ 10
#Gautas atsakymas iÅ¡ tiesÄ—s lygties: 23.305 yra labai panaÅ¡us Ä¯ spÄ—jamÄ… atsakymÄ…: 23.30998 
yReg =  k * 10 + b
yReg
predict(lm(y~x), data.frame(x=10))

#2.3
#Nurodyta p-value reikÅ¡mÄ— yra daug maÅ¾esnÄ— uÅ¾ duotÄ… reikÅ¡mingumo lygmenÄ¯ 0.05
#NulinÄ™ hipotezÄ™, kad tiesinÄ—s lygties krypties koeficientas lygus nuliui atmentame, todÄ—l galime pereiti prie alternatyviosios hipotezÄ—s
summary(lm(y~x))

#2.4
#TiesinÄ— regresijos determinacijos koeficinetas lygus 0.9798
#Å is koeficientas yra labai arti 1, todÄ—l tiesinÄ—s regresijos modelis yra tinkamas Å¡iems duomenims
summary(lm(y~x))

#2.5
#IÅ¡ histogramos galima matyti, kad liekanÅ³ skirstinys primena normalÅ³jÄ¯ skirstinÄ¯
#IÅ¡ grafiko galima matyti, kad skirstinys yra homoskedatiÅ¡kas.
e = y - k*x - b
plot(e)
hist(e)

#2.6
#P-value = 0.4009 reikÅ¡mÄ— yra didesnÄ— uÅ¾ ğ›¼ = 0.01, todÄ—l neatmetame statistinÄ—s hipotezÄ—s, jog regresijos liekanÅ³ skirstinys yra normalusis.
library(MASS)
fitdistr(e, 'normal')
ks.test(e, 'pnorm', 0, 0.0299946010 )